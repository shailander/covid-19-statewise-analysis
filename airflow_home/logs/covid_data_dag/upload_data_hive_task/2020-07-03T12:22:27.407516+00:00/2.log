[2020-07-10 13:43:57,317] {taskinstance.py:669} INFO - Dependencies all met for <TaskInstance: covid_data_dag.upload_data_hive_task 2020-07-03T12:22:27.407516+00:00 [queued]>
[2020-07-10 13:43:57,325] {taskinstance.py:669} INFO - Dependencies all met for <TaskInstance: covid_data_dag.upload_data_hive_task 2020-07-03T12:22:27.407516+00:00 [queued]>
[2020-07-10 13:43:57,325] {taskinstance.py:879} INFO - 
--------------------------------------------------------------------------------
[2020-07-10 13:43:57,325] {taskinstance.py:880} INFO - Starting attempt 2 of 2
[2020-07-10 13:43:57,325] {taskinstance.py:881} INFO - 
--------------------------------------------------------------------------------
[2020-07-10 13:43:57,332] {taskinstance.py:900} INFO - Executing <Task(PythonOperator): upload_data_hive_task> on 2020-07-03T12:22:27.407516+00:00
[2020-07-10 13:43:57,334] {standard_task_runner.py:53} INFO - Started process 76180 to run task
[2020-07-10 13:43:57,378] {logging_mixin.py:112} INFO - Running %s on host %s <TaskInstance: covid_data_dag.upload_data_hive_task 2020-07-03T12:22:27.407516+00:00 [running]> localhost.localdomain
[2020-07-10 13:44:04,805] {taskinstance.py:1145} ERROR - LOAD DATA input path does not exist: hdfs://localhost:9000/user/nineleaps/covid_data.csv;
Traceback (most recent call last):
  File "/home/nineleaps/PycharmProjects/ProjectX/venv/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 983, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/nineleaps/PycharmProjects/ProjectX/venv/lib/python3.8/site-packages/airflow/operators/python_operator.py", line 113, in execute
    return_value = self.execute_callable()
  File "/home/nineleaps/PycharmProjects/ProjectX/venv/lib/python3.8/site-packages/airflow/operators/python_operator.py", line 118, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/nineleaps/PycharmProjects/ProjectX/airflow_home/plugins/etl_tasks.py", line 65, in upload_data_to_hive
    main()
  File "/home/nineleaps/PycharmProjects/ProjectX/airflow_home/plugins/hive_upload.py", line 23, in main
    spark.sql("""load data inpath
  File "/home/nineleaps/PycharmProjects/ProjectX/venv/lib/python3.8/site-packages/pyspark/sql/session.py", line 646, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)
  File "/home/nineleaps/PycharmProjects/ProjectX/venv/lib/python3.8/site-packages/py4j/java_gateway.py", line 1304, in __call__
    return_value = get_return_value(
  File "/home/nineleaps/PycharmProjects/ProjectX/venv/lib/python3.8/site-packages/pyspark/sql/utils.py", line 137, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.AnalysisException: LOAD DATA input path does not exist: hdfs://localhost:9000/user/nineleaps/covid_data.csv;
[2020-07-10 13:44:04,818] {taskinstance.py:1174} INFO - All retries failed; marking task as FAILED.dag_id=covid_data_dag, task_id=upload_data_hive_task, execution_date=20200703T122227, start_date=20200710T081357, end_date=20200710T081404
[2020-07-10 13:44:07,313] {logging_mixin.py:112} INFO - [2020-07-10 13:44:07,313] {local_task_job.py:103} INFO - Task exited with return code 1
